{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Natural Language Processing (NLP) using a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "imdb.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text, y_train = imdb.load_data(train=True)\n",
    "x_test_text, y_test = imdb.load_data(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set size:  25000\n",
      "Test-set size:   25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-set size: \", len(x_train_text))\n",
    "print(\"Test-set size:  \", len(x_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = x_train_text + x_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they\\'ll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it\\'s like to be homeless? That is Goddard Bolt\\'s lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet\\'s on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can\\'t step off the sidewalk. He\\'s given the nickname Pepto by a vagrant after it\\'s written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They\\'re survivors. Bolt isn\\'t. He\\'s not used to reaching mutual agreements like he once did when being rich where it\\'s fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn\\'t necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks\\' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it\\'s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don\\'t know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I\\'m a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_words is None:\n",
    "    num_words = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then inspect the vocabulary that has been gathered by the tokenizer. This is ordered by the number of occurrences of the words in the data-set. These integer-numbers are called word indices or \"tokens\" because they uniquely identify each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'psychoactive': 50949,\n",
       " \"1690's\": 68757,\n",
       " 'tooney': 67545,\n",
       " \"reality's\": 64089,\n",
       " 'guillotines': 32430,\n",
       " 'haruhiko': 56932,\n",
       " \"cunningham's\": 30128,\n",
       " 'catalunya': 43979,\n",
       " 'dawes': 71023,\n",
       " 'keyboardists': 61866,\n",
       " '50ft': 94995,\n",
       " 'serbia': 21033,\n",
       " 'katzman': 41248,\n",
       " 'flitted': 82863,\n",
       " 'nazies': 116034,\n",
       " 'enhancement': 24624,\n",
       " 'anisio': 37536,\n",
       " 'aavjo': 75803,\n",
       " 'dissimilar': 20788,\n",
       " 'consented': 44171,\n",
       " 'prezzo': 47134,\n",
       " \"sibrel's\": 45838,\n",
       " 'hilarity': 5652,\n",
       " '32nd': 77148,\n",
       " \"herb's\": 122218,\n",
       " 'nominees': 15889,\n",
       " 'ojo': 102756,\n",
       " 'chinaman': 38483,\n",
       " 'roaming': 12620,\n",
       " 'remonstration': 96473,\n",
       " 'repeats': 7978,\n",
       " 'grossbach': 108986,\n",
       " 'sadiki': 56148,\n",
       " 'reinstate': 59553,\n",
       " 'limped': 34173,\n",
       " 'reassertion': 94106,\n",
       " 'kya': 49101,\n",
       " 'snails”': 120611,\n",
       " 'amorós': 57187,\n",
       " 'incentives': 46952,\n",
       " 'ouies': 116389,\n",
       " \"pros's\": 87938,\n",
       " 'krusty': 64264,\n",
       " 'categorical': 61067,\n",
       " 'engulf': 22892,\n",
       " '\\x84old': 74886,\n",
       " \"rybak's\": 107186,\n",
       " 'ohwon': 31078,\n",
       " 'stroesser': 72354,\n",
       " 'faulker': 110788,\n",
       " 'horrortitles': 87140,\n",
       " 'behold': 5823,\n",
       " 'torres': 27669,\n",
       " 'ebts': 46761,\n",
       " \"svendsen's\": 63443,\n",
       " 'jokester': 56737,\n",
       " 'daley': 30618,\n",
       " 'playas': 116281,\n",
       " 'occupying': 19145,\n",
       " \"solondz's\": 55491,\n",
       " 'preadolescence': 77320,\n",
       " 'associated': 3603,\n",
       " 'apt': 7490,\n",
       " 'thundering': 30441,\n",
       " 'refinement': 23260,\n",
       " 'sergi': 75436,\n",
       " 'waltari': 38541,\n",
       " \"'effects\": 90291,\n",
       " \"'cover\": 25390,\n",
       " \"risible'n'ridiculous\": 78655,\n",
       " 'rhpot': 103756,\n",
       " 'sera': 14807,\n",
       " \"'looking\": 69065,\n",
       " 'boondock': 30704,\n",
       " \"langford'\": 114719,\n",
       " 'kidotai': 104522,\n",
       " 'mouthfuls': 123062,\n",
       " 'rize': 63657,\n",
       " 'birch': 20499,\n",
       " 'robsahm': 57468,\n",
       " '1907': 39216,\n",
       " \"franko's\": 87980,\n",
       " 'loudness': 46796,\n",
       " 'lerman': 25831,\n",
       " 'pasted': 12519,\n",
       " 'minutes\\x97': 94501,\n",
       " \"amc's\": 64520,\n",
       " 'instable': 105221,\n",
       " 'pulcherie': 78479,\n",
       " 'bliss': 9755,\n",
       " 'predates': 20153,\n",
       " 'meyerling': 36117,\n",
       " 'veen': 89126,\n",
       " 'icegun': 92790,\n",
       " 'unmask': 39020,\n",
       " 'guss': 56601,\n",
       " 'personal': 939,\n",
       " 'kemosabe': 69883,\n",
       " 'takahiro': 67943,\n",
       " '618': 94953,\n",
       " 'himnan': 124192,\n",
       " 'blethyn': 14025,\n",
       " 'honkey': 102308,\n",
       " \"mork's\": 97034,\n",
       " \"commandment's\": 102185,\n",
       " 'athens': 22330,\n",
       " 'barsat': 77395,\n",
       " 'complimentary': 20991,\n",
       " \"crowd's\": 40081,\n",
       " 'specialize': 39348,\n",
       " 'cryogenic': 33135,\n",
       " 'unfortuneatley': 95581,\n",
       " 'lorri': 57865,\n",
       " 'spiritism': 95892,\n",
       " 'reid': 7239,\n",
       " 'fazed': 119239,\n",
       " 'rocaille': 91777,\n",
       " 'papa': 15607,\n",
       " 'oder': 37671,\n",
       " 'whoopi': 7531,\n",
       " 'outsmarting': 118729,\n",
       " 'uruk': 76254,\n",
       " \"moose'\": 94684,\n",
       " 'parisy': 54790,\n",
       " 'amateur': 2457,\n",
       " 'clanking': 70367,\n",
       " \"mcshane's\": 57403,\n",
       " \"flamengo's\": 85077,\n",
       " 'detraction': 37313,\n",
       " \"pax'\": 108090,\n",
       " 'blairwitch': 71222,\n",
       " 'ralphy': 54158,\n",
       " 'micah': 57688,\n",
       " 'immensely': 4195,\n",
       " 'pip': 12719,\n",
       " 'intersplicing': 54284,\n",
       " 'hollowood': 108444,\n",
       " 'barugon': 56086,\n",
       " 'circuses': 42739,\n",
       " 'sincere': 4500,\n",
       " 'ogdari': 111225,\n",
       " 'pickaxes': 57173,\n",
       " 'adventists': 38836,\n",
       " 'commander': 4832,\n",
       " \"'custard\": 103470,\n",
       " 'kinlaw': 75286,\n",
       " 'ironside\\x97usually': 116463,\n",
       " \"b'c\": 114890,\n",
       " 'intergenerational': 40019,\n",
       " \"mlk's\": 103898,\n",
       " 'auken': 122008,\n",
       " 'fealty': 118922,\n",
       " 'reggiani': 110150,\n",
       " 'sandu': 34821,\n",
       " 'clooney': 6376,\n",
       " 'humor\\x85of': 87152,\n",
       " 'trivial': 7054,\n",
       " 'bagging': 37221,\n",
       " \"'enron'\": 124132,\n",
       " \"'goy'\": 114116,\n",
       " 'weg': 95542,\n",
       " 'false': 2639,\n",
       " 'epitomized': 20609,\n",
       " \"jbl's\": 84275,\n",
       " 'donato': 59186,\n",
       " 'eda': 68818,\n",
       " \"dumont's\": 77647,\n",
       " 'spindash': 116425,\n",
       " 'medias': 50593,\n",
       " 'mailman': 32237,\n",
       " 'palminterri': 64943,\n",
       " 'yamasato': 51857,\n",
       " 'cloaked': 19798,\n",
       " 'crackers': 12947,\n",
       " 'giggled': 26055,\n",
       " 'blankets': 43352,\n",
       " 'renewed': 12891,\n",
       " 'kushrenada': 51691,\n",
       " 'zeroing': 88524,\n",
       " \"blake's\": 21677,\n",
       " 'lionsault': 57303,\n",
       " \"defence'\": 40390,\n",
       " 'pantsuit': 61111,\n",
       " \"area's\": 54599,\n",
       " 'biciclette': 103086,\n",
       " 'begin': 904,\n",
       " 'fagot': 121256,\n",
       " 'disrobe': 26806,\n",
       " 'dorando': 109901,\n",
       " 'pup': 21406,\n",
       " 'beastly': 27102,\n",
       " 'poopchev': 87379,\n",
       " 'emilyfussell': 98933,\n",
       " 'crane': 11518,\n",
       " \"al'a\": 102049,\n",
       " 'tressa': 61489,\n",
       " 'meital': 119708,\n",
       " 'gene': 2264,\n",
       " 'showcasing': 13524,\n",
       " 'return': 1037,\n",
       " 'cleve': 75657,\n",
       " 'quake': 24047,\n",
       " \"'cannes'\": 86449,\n",
       " 'doubtfire': 44502,\n",
       " 'magnate': 23927,\n",
       " 'cliqued': 100612,\n",
       " 'hero’s': 73702,\n",
       " 'royalties': 25647,\n",
       " 'mcgavin': 11904,\n",
       " 'sweetish': 64650,\n",
       " 'pusses': 94990,\n",
       " 'jaipur': 54203,\n",
       " 'ciarán': 61877,\n",
       " 'furtermore': 108697,\n",
       " \"warburton's\": 51885,\n",
       " 'conned': 16101,\n",
       " \"ranma's\": 52028,\n",
       " 'duds': 15830,\n",
       " 'freewill': 55017,\n",
       " 'seminar': 28111,\n",
       " 'chacha': 84896,\n",
       " 'herodotus': 114865,\n",
       " 'loosed': 39692,\n",
       " 'overlie': 118522,\n",
       " 'briganza': 105502,\n",
       " 'comfy': 21489,\n",
       " 'cheapo': 15670,\n",
       " 'punkette': 62302,\n",
       " 'jangles': 94929,\n",
       " \"whispers'\": 91310,\n",
       " \"\\x9160's\": 112494,\n",
       " 'danns': 117826,\n",
       " \"encounter'\": 77498,\n",
       " 'beaten': 3565,\n",
       " \"jillian's\": 66881,\n",
       " 'mini': 2263,\n",
       " 'combine': 4519,\n",
       " 'horsie': 89481,\n",
       " 'signaled': 29976,\n",
       " 'wrested': 91692,\n",
       " \"demille's\": 24134,\n",
       " 'dateable': 107812,\n",
       " \"'duncan'\": 104960,\n",
       " \"'defects'\": 112205,\n",
       " 'vastly': 5886,\n",
       " 'apothacary': 108743,\n",
       " 'conversational': 30050,\n",
       " 'audial': 69929,\n",
       " 'nonviolence': 77298,\n",
       " 'cartland': 41305,\n",
       " 'feagin': 47728,\n",
       " 'admitting': 15601,\n",
       " 'bettys': 69814,\n",
       " 'rantings': 25165,\n",
       " \"luzon's\": 79536,\n",
       " 'gobbleygook': 115890,\n",
       " \"minorities'\": 54359,\n",
       " \"lincoln's\": 17868,\n",
       " 'featherweight': 43248,\n",
       " 'pyschologist': 98735,\n",
       " 'oversight': 22040,\n",
       " 'materialists': 65626,\n",
       " 'fuckwood': 96708,\n",
       " '30th': 21499,\n",
       " 'gordons': 63910,\n",
       " 'silmerman': 108170,\n",
       " 'twas': 58513,\n",
       " 'game': 465,\n",
       " 'principality': 59596,\n",
       " 'thighs\\x85': 113077,\n",
       " 'shelly': 14349,\n",
       " 'onto': 1609,\n",
       " 'testaments': 48820,\n",
       " 'bachstage': 84866,\n",
       " 'antaius': 55559,\n",
       " 'kevin': 1650,\n",
       " 'nightime': 99406,\n",
       " 'fuming': 30673,\n",
       " '0tt': 98937,\n",
       " 'arms': 2607,\n",
       " 'messages': 3647,\n",
       " 'jotted': 37461,\n",
       " 'rupaul': 71630,\n",
       " \"landing'\": 66291,\n",
       " 'munched': 32287,\n",
       " 'invisível': 115278,\n",
       " 'prodigy': 17645,\n",
       " 'perfomrance': 104599,\n",
       " 'scoffs': 40352,\n",
       " 'biz': 12815,\n",
       " 'antagonism': 31937,\n",
       " 'tritingnant': 111089,\n",
       " 'brobdingnag': 69069,\n",
       " \"gein'\": 117909,\n",
       " 'thud': 24253,\n",
       " 'poke': 9173,\n",
       " \"fundamentalist's\": 118700,\n",
       " 'filmy': 24766,\n",
       " 'undergo': 13532,\n",
       " 'grovers': 119972,\n",
       " 'atlantic': 7724,\n",
       " 'catweazle': 61211,\n",
       " \"'escapist'\": 123203,\n",
       " 'treacherous': 11866,\n",
       " 'æon': 35646,\n",
       " 'episode\\x97here': 81713,\n",
       " 'sturla': 60383,\n",
       " 'trolley': 31263,\n",
       " \"'mirrors'\": 94386,\n",
       " 'sylvio': 60862,\n",
       " \"geisha's\": 85092,\n",
       " 'chimayo': 54073,\n",
       " 'flickering': 15500,\n",
       " 'astaire': 5560,\n",
       " '0230': 121405,\n",
       " 'cate': 20936,\n",
       " 'adolescents': 15323,\n",
       " 'incarnates': 105274,\n",
       " 'manure': 14290,\n",
       " 'unhelpful': 38058,\n",
       " 'rarely': 1703,\n",
       " \"hellman's\": 42800,\n",
       " \"hui's\": 107655,\n",
       " 'obtained': 13786,\n",
       " 'hotfoots': 120526,\n",
       " \"zizek's\": 17777,\n",
       " 'explosivo': 55713,\n",
       " 'rafter': 86750,\n",
       " \"reader's\": 21323,\n",
       " 'ishida': 110222,\n",
       " 'maddona': 90946,\n",
       " 'jaffe': 17067,\n",
       " 'carlylegroup': 110181,\n",
       " \"spheeris's\": 84663,\n",
       " \"yimou's\": 42094,\n",
       " \"colors'\": 74522,\n",
       " 'remarks': 5226,\n",
       " 'abydos': 42742,\n",
       " \"'cube'\": 48255,\n",
       " 'reimbursable': 99699,\n",
       " 'gory': 2078,\n",
       " 'newby': 54714,\n",
       " 'outlasts': 99681,\n",
       " 'intciting': 104718,\n",
       " 'dogville': 28066,\n",
       " \"felix's\": 21469,\n",
       " 'pandey': 28951,\n",
       " 'artery': 50456,\n",
       " 'destroy': 2222,\n",
       " 'maximizing': 117503,\n",
       " \"nothin'\": 58528,\n",
       " \"krisner's\": 88883,\n",
       " 'analyst': 19604,\n",
       " \"pandora's\": 23184,\n",
       " 'whiteley': 82357,\n",
       " 'concealing': 29444,\n",
       " 'sarlac': 83820,\n",
       " 'korman': 25356,\n",
       " \"matrix'\": 24806,\n",
       " 'acrossed': 109618,\n",
       " \"'indian\": 60579,\n",
       " 'pasadena': 32296,\n",
       " 'ubc': 64898,\n",
       " 'unequipped': 120027,\n",
       " 'kilter': 13828,\n",
       " \"spradling's\": 94180,\n",
       " 'brooked': 89366,\n",
       " 'sheppird': 113035,\n",
       " \"'effing'\": 77865,\n",
       " 'ritchkoff': 70478,\n",
       " 'greeting': 16708,\n",
       " 'little': 120,\n",
       " 'confess\\x85': 112404,\n",
       " \"'nasaan\": 71484,\n",
       " 'upteenth': 114466,\n",
       " 'bemoaned': 65631,\n",
       " 'executors': 84791,\n",
       " 'eventualy': 98569,\n",
       " 'roasting': 26759,\n",
       " 'shampoo': 22437,\n",
       " 'scattergun': 89401,\n",
       " 'bantha': 61360,\n",
       " 'cavewoman': 64454,\n",
       " 'thoughtless': 21452,\n",
       " 'plasticine': 30867,\n",
       " 'reversioned': 105903,\n",
       " 'akhras': 42115,\n",
       " 'cantinas': 97492,\n",
       " \"character's\": 1807,\n",
       " 'screenwriters': 6072,\n",
       " \"dream'n\": 104948,\n",
       " 'motiveless': 42034,\n",
       " 'miwi': 102560,\n",
       " 'uncanny': 7348,\n",
       " \"ideal'\": 83891,\n",
       " 'jacka': 114648,\n",
       " 'thorny': 40242,\n",
       " 'y12': 70174,\n",
       " 'spagethi': 121167,\n",
       " 'bobbity': 76956,\n",
       " 'scaryt': 89284,\n",
       " 'much': 72,\n",
       " \"osbourne's\": 99380,\n",
       " 'xzptdtphwdm': 79440,\n",
       " 'reasonn': 118429,\n",
       " 'governor': 8612,\n",
       " 'relayed': 26215,\n",
       " 'lowlights': 64790,\n",
       " \"generation's\": 36914,\n",
       " 'sommer': 26477,\n",
       " 'rodríguez': 64652,\n",
       " 'ever': 123,\n",
       " 'observe': 7372,\n",
       " \"'fun'\": 25083,\n",
       " 'ransohoff': 74500,\n",
       " \"'sorry'\": 56741,\n",
       " 'finlay': 12730,\n",
       " \"l'ami\": 49049,\n",
       " 'quartermaine': 34520,\n",
       " 'impalement': 50866,\n",
       " \"rices's\": 101193,\n",
       " 'clément': 52543,\n",
       " 'desserts': 15128,\n",
       " 'maltin': 11484,\n",
       " 'wopat': 46410,\n",
       " 'abrigado': 106363,\n",
       " 'belongs': 3225,\n",
       " 'bromidic': 80089,\n",
       " 'wolverine': 14978,\n",
       " \"kunstler's\": 106972,\n",
       " \"york's\": 12560,\n",
       " 'basing': 18882,\n",
       " \"willis's\": 101482,\n",
       " 'lennie': 48490,\n",
       " 'proberbly': 103672,\n",
       " \"'dutch'\": 110446,\n",
       " 'boringlane': 91847,\n",
       " 'clichée': 46550,\n",
       " 'sounder': 68677,\n",
       " 'desplechin': 52482,\n",
       " 'burnham': 118740,\n",
       " 'healed': 16414,\n",
       " 'vid': 30604,\n",
       " 'ferland': 28384,\n",
       " 'prostrate': 43384,\n",
       " 'catty': 24055,\n",
       " \"cowboys'\": 63170,\n",
       " 'soppy': 17574,\n",
       " 'open': 849,\n",
       " 'remixes': 53057,\n",
       " 'cuz': 13717,\n",
       " \"l'elisir\": 72412,\n",
       " \"'care\": 74524,\n",
       " 'entereth': 81348,\n",
       " 'pta': 32953,\n",
       " 'benshi': 73861,\n",
       " 'pooish': 105843,\n",
       " 'plane’s': 120514,\n",
       " 'plz': 33266,\n",
       " \"warhol's'\": 84654,\n",
       " \"augustus'\": 113917,\n",
       " 'slapshot': 68602,\n",
       " \"crockett's\": 92305,\n",
       " 'vanishing': 11609,\n",
       " 'suuden': 120354,\n",
       " 'harem': 19488,\n",
       " \"harvest'\": 58834,\n",
       " 'merde': 33289,\n",
       " 'fastow': 119323,\n",
       " 'smatter': 106687,\n",
       " 'helicopters': 10736,\n",
       " 'whitmore': 30081,\n",
       " 'flooded': 13927,\n",
       " \"'protector'\": 87148,\n",
       " 'whateverist': 114975,\n",
       " 'casisa': 55177,\n",
       " 'zizola': 59040,\n",
       " 'grotesuque': 66494,\n",
       " 'iain': 27848,\n",
       " \"thing'\": 20859,\n",
       " \"donnison's\": 38296,\n",
       " 'galens': 90983,\n",
       " 'cephallonia': 68400,\n",
       " 'iliad': 29041,\n",
       " 'letterhead': 99260,\n",
       " 'secularity': 74764,\n",
       " 'poisoned': 11708,\n",
       " \"mcelwee's\": 52159,\n",
       " 'bendorf': 113424,\n",
       " 'dcoms': 48033,\n",
       " 'hannibal': 9827,\n",
       " 'candians': 86211,\n",
       " \"hird's\": 123458,\n",
       " 'subcortical': 100405,\n",
       " 'não': 85330,\n",
       " 'cuddy': 65133,\n",
       " 'linderby': 41128,\n",
       " 'resilient': 27037,\n",
       " \"express'\": 61700,\n",
       " 'inthe': 101501,\n",
       " \"hellenlotter's\": 97470,\n",
       " 'uncontrolled': 31605,\n",
       " 'monica': 7253,\n",
       " \"entertainment's\": 25616,\n",
       " 'hamza': 114138,\n",
       " 'inward': 28637,\n",
       " 'deadlier': 35953,\n",
       " 'tecnically': 116712,\n",
       " 'bridger': 38580,\n",
       " 'martian': 9390,\n",
       " 'grr': 114090,\n",
       " 'siphon': 66093,\n",
       " 'escapade': 20363,\n",
       " 'loneley': 114040,\n",
       " 'snazzy': 30818,\n",
       " 'conviction': 5670,\n",
       " 'loners': 25477,\n",
       " 'halucinations': 115628,\n",
       " 'stuey': 22693,\n",
       " \"'old\": 23023,\n",
       " 'unfulfilled': 14441,\n",
       " 'gibbon': 67167,\n",
       " 'romanao': 104618,\n",
       " 'beecham': 50041,\n",
       " 'vinaigrette': 114003,\n",
       " 'fraternization': 58469,\n",
       " \"cafe'\": 117584,\n",
       " 'americanos': 101791,\n",
       " 'deary': 52333,\n",
       " 'teensy': 48343,\n",
       " 'virulently': 80859,\n",
       " 'lantos': 64544,\n",
       " 'rohauer': 79170,\n",
       " 'foreplay': 32569,\n",
       " 'snarling': 19327,\n",
       " 'pleasures': 7606,\n",
       " 'european\\x97but': 113987,\n",
       " \"'children'\": 62022,\n",
       " \"morrill's\": 57805,\n",
       " '1080': 119277,\n",
       " 'enormous': 4080,\n",
       " 'sinuously': 81254,\n",
       " 'algiers': 23914,\n",
       " 'tock': 51252,\n",
       " 'joanna': 6898,\n",
       " 'minutes\\x97is': 122697,\n",
       " \"bellini'\": 60462,\n",
       " 'annoys': 9893,\n",
       " \"ferrari's\": 45215,\n",
       " 'vagrant': 38711,\n",
       " 'bardot': 17186,\n",
       " 'brownlow': 76013,\n",
       " 'ethnic': 5431,\n",
       " 'conservationists': 74264,\n",
       " 'magalhães': 49098,\n",
       " 'grumping': 96376,\n",
       " 'mazurki': 34931,\n",
       " 'arjuna': 91159,\n",
       " 'disjointed': 3806,\n",
       " 'wajda': 18870,\n",
       " 'yakima': 21115,\n",
       " 'sprouting': 71291,\n",
       " 'saleslady': 79688,\n",
       " 'formulas': 17643,\n",
       " 'buick': 36510,\n",
       " 'ww2': 7509,\n",
       " 'climactically': 118648,\n",
       " 'stupefied': 49440,\n",
       " \"bacchan's\": 114477,\n",
       " 'abagail': 124189,\n",
       " \"om'\": 104538,\n",
       " 'superteen': 109875,\n",
       " 'lindbergh': 16057,\n",
       " 'odd\\x85': 83701,\n",
       " \"largo'\": 110101,\n",
       " 'blinders': 40162,\n",
       " 'cleans': 15736,\n",
       " 'muñoz': 89472,\n",
       " 'halted': 39617,\n",
       " \"toyoda's\": 54917,\n",
       " \"'short\": 54145,\n",
       " '\\xa0which': 115011,\n",
       " \"'confronter'\": 101536,\n",
       " \"a's\": 34461,\n",
       " 'manky': 63078,\n",
       " 'beasties': 36342,\n",
       " 'rendez': 57080,\n",
       " 'churchill': 12020,\n",
       " 'shindig': 58552,\n",
       " 'ordinarily': 16771,\n",
       " 'takaragaike': 117233,\n",
       " 'ursine': 78575,\n",
       " 'ilse': 51848,\n",
       " 'neighborhood': 3463,\n",
       " 'talk': 705,\n",
       " 'abstains': 81299,\n",
       " 'stepmotherhood': 61003,\n",
       " 'tedeschi': 123606,\n",
       " 'struck': 3245,\n",
       " 'sherry': 13043,\n",
       " 'nobuaki': 101872,\n",
       " 'quibbling': 61018,\n",
       " 'misanthropist': 48229,\n",
       " 'bustling': 23054,\n",
       " 'usefull': 96274,\n",
       " 'incarcerations': 93130,\n",
       " \"strip's\": 57338,\n",
       " \"katsumi's\": 76564,\n",
       " 'hoofer': 37156,\n",
       " 'cortes': 23231,\n",
       " \"liebermann's\": 91396,\n",
       " 'umaga': 94552,\n",
       " 'duenas': 38473,\n",
       " 'miniature': 9844,\n",
       " 'consciousness': 5950,\n",
       " 'guaging': 73108,\n",
       " \"posey's\": 25873,\n",
       " 'glimpse': 3309,\n",
       " 'bright': 1916,\n",
       " 'formalità': 121940,\n",
       " 'spitted': 88125,\n",
       " \"there'd\": 19573,\n",
       " \"\\x8ei\\x9eek's\": 33919,\n",
       " 'origination': 93480,\n",
       " 'inorganic': 67821,\n",
       " 'llyods': 115428,\n",
       " 'calahan': 83693,\n",
       " 'inveighs': 113930,\n",
       " 'kittelsen': 106848,\n",
       " 'dafter': 123583,\n",
       " 'lifeless': 5649,\n",
       " 'dc': 10312,\n",
       " 'depths': 5947,\n",
       " 'azz': 105041,\n",
       " 'higgins': 15000,\n",
       " 'emissions': 117392,\n",
       " 'leaders': 6243,\n",
       " 'infrared': 41781,\n",
       " 'seemingly': 1504,\n",
       " 'montez': 43713,\n",
       " \"'mutton\": 112656,\n",
       " 'apocalypse': 4896,\n",
       " 'dumitru': 64890,\n",
       " \"airplane's\": 107103,\n",
       " 'bosnian': 23343,\n",
       " 'unleashes': 15818,\n",
       " \"'throwing\": 60814,\n",
       " 'nikos': 46923,\n",
       " 'detracts': 11047,\n",
       " 'everyway': 40340,\n",
       " \"tasker's\": 71341,\n",
       " 'fastest': 20233,\n",
       " 'justness': 82596,\n",
       " \"o'halloran\": 46720,\n",
       " 'jasmin': 40612,\n",
       " 'belied': 52198,\n",
       " 'covey': 45033,\n",
       " 'peat': 53953,\n",
       " 'libeled': 55624,\n",
       " 'valeri': 53718,\n",
       " \"lemmon's\": 17699,\n",
       " 'está': 61738,\n",
       " 'hasak': 118255,\n",
       " 'kirsted': 85552,\n",
       " 'hau': 109893,\n",
       " 'mcpoodle': 42309,\n",
       " 'reserving': 59729,\n",
       " \"vinci's\": 47432,\n",
       " 'conservation': 39565,\n",
       " 'fairbrass': 48423,\n",
       " 'stoped': 65967,\n",
       " \"teodoro's\": 52948,\n",
       " 'experiments': 4928,\n",
       " 'inebriate': 69486,\n",
       " \"3'\": 21555,\n",
       " 'dogmatically': 62469,\n",
       " 'cking': 21467,\n",
       " 'xxx2': 93579,\n",
       " 'lunacies': 90900,\n",
       " 'zey': 38480,\n",
       " 'repainted': 56771,\n",
       " 'yello': 75024,\n",
       " 'matlock': 19929,\n",
       " 'whoppers': 53427,\n",
       " 'guises': 34976,\n",
       " 'naab': 81432,\n",
       " \"gates'\": 93703,\n",
       " 'value\\x85': 112006,\n",
       " '1891': 42234,\n",
       " 'mildest': 45646,\n",
       " 'garages': 60713,\n",
       " \"sellers's\": 90027,\n",
       " 'cliches': 6345,\n",
       " 'duckling': 10276,\n",
       " 'skinkons': 77926,\n",
       " 'harrelson': 17077,\n",
       " 'chopped': 6230,\n",
       " 'shockfest': 114732,\n",
       " 'slammed': 18166,\n",
       " 'letal': 117579,\n",
       " 'paratroopers': 54778,\n",
       " \"gabby's\": 97620,\n",
       " 'keypunch': 97149,\n",
       " 'oshea': 81144,\n",
       " 'chiefs': 25669,\n",
       " 'subjective': 10605,\n",
       " 'hoot': 5543,\n",
       " 'lachman': 93446,\n",
       " 'colombians': 87421,\n",
       " 'chardonay': 118629,\n",
       " 'woodenly': 38070,\n",
       " 'zekeria': 47566,\n",
       " 'mclane': 47109,\n",
       " 'beelzebub': 115779,\n",
       " 'ciro': 118251,\n",
       " 'auh': 72720,\n",
       " 'prism': 41739,\n",
       " 'gorier': 26327,\n",
       " 'casted': 9239,\n",
       " \"chile's\": 52796,\n",
       " \"risques'\": 101124,\n",
       " 'carnevores': 103213,\n",
       " 'squall': 38702,\n",
       " 'bittersweet': 7565,\n",
       " 'hirjee': 60388,\n",
       " 'maize': 26459,\n",
       " \"sachs'\": 39431,\n",
       " 'film\\x97currently': 103208,\n",
       " 'kusanagi': 69953,\n",
       " 'gurnemanz': 36098,\n",
       " 'gregorowicz': 114474,\n",
       " 'penitentiaries': 57651,\n",
       " 'rioters': 49949,\n",
       " 'she’s': 56475,\n",
       " \"french's\": 107136,\n",
       " '\\x91from': 117179,\n",
       " 'cowards': 25106,\n",
       " 'rosenmüller': 91765,\n",
       " 'brainiac': 29761,\n",
       " 'kenard': 113219,\n",
       " 'tracklist': 88537,\n",
       " 'unfooled': 119501,\n",
       " 'coherency': 21020,\n",
       " 'docos': 75455,\n",
       " \"beyond'\": 46989,\n",
       " 'citizens': 4548,\n",
       " 'zucovic': 57708,\n",
       " 'hayenga': 48651,\n",
       " 'mere': 2807,\n",
       " 'shamsul': 105392,\n",
       " 'relationsip': 74832,\n",
       " 'machatý': 60119,\n",
       " 'influences': 8094,\n",
       " 'thunderbolts': 35639,\n",
       " 'wunderbar': 84994,\n",
       " 'thinnes': 16004,\n",
       " 'foremans': 77242,\n",
       " 'disappeared': 4406,\n",
       " 'baaaack': 124135,\n",
       " 'sawa': 23388,\n",
       " 'wises': 60365,\n",
       " \"erroll's\": 50640,\n",
       " 'tewes': 69914,\n",
       " 'rumiko': 52029,\n",
       " 'impelled': 39071,\n",
       " 'easier': 3443,\n",
       " \"tide'\": 78504,\n",
       " \"highland's\": 69827,\n",
       " \"'signifiant'\": 117209,\n",
       " \"'stargate\": 56389,\n",
       " 'manner': 1419,\n",
       " 'arty': 6940,\n",
       " 'nagasaki': 21192,\n",
       " 'auditioners': 116319,\n",
       " 'barbarella': 29231,\n",
       " 'easily': 736,\n",
       " 'schmalz': 104297,\n",
       " 'disenchanting': 102176,\n",
       " 'mystical': 7460,\n",
       " 'casually': 8141,\n",
       " 'commentary': 1646,\n",
       " \"wilding's\": 68165,\n",
       " 'baans': 71762,\n",
       " 'that': 12,\n",
       " 'sarcastically': 29838,\n",
       " 'crumbs': 36222,\n",
       " 'tog': 100168,\n",
       " 'ebon': 93271,\n",
       " 'reigne': 80818,\n",
       " 'bobbitt': 90597,\n",
       " \"tomlinson's\": 60370,\n",
       " 'chicory': 83383,\n",
       " 'raat': 34246,\n",
       " 'dumbfounded': 18325,\n",
       " 'cattleman': 40288,\n",
       " \"dramatic's\": 112521,\n",
       " 'litja': 47674,\n",
       " 'lemac': 91813,\n",
       " 'incapacitate': 51159,\n",
       " 'decade': 2121,\n",
       " 'deterr': 73748,\n",
       " 'runestone': 109497,\n",
       " 'toggles': 67679,\n",
       " \"'stanley\": 58250,\n",
       " 'brookmyre': 66065,\n",
       " \"spackler's\": 92555,\n",
       " \"barney's\": 25363,\n",
       " 'greenery': 44305,\n",
       " 'vilest': 40600,\n",
       " 'epstein': 34547,\n",
       " 'bestsellers': 54045,\n",
       " 'convinces': 6212,\n",
       " 'lagerlöf': 38499,\n",
       " 'drift': 8503,\n",
       " 'tansy': 90181,\n",
       " 'ganzel': 46884,\n",
       " 'donny': 33566,\n",
       " 'karsis': 72612,\n",
       " 'vigour': 32026,\n",
       " 'loudly': 9836,\n",
       " 'numbnuts': 114899,\n",
       " 'iciness': 74490,\n",
       " 'rockythebear': 72436,\n",
       " 'm3': 115501,\n",
       " 'voyna': 46729,\n",
       " 'minstrels': 52813,\n",
       " 'unflappable': 36795,\n",
       " 'greig': 123689,\n",
       " \"'soap'\": 115221,\n",
       " 'bobulova': 55163,\n",
       " 'saved': 2012,\n",
       " 'martinelli': 42350,\n",
       " 'soupçon': 91068,\n",
       " 'brezina': 100589,\n",
       " 'jonh': 63730,\n",
       " \"'house\": 22007,\n",
       " 'baaa': 122179,\n",
       " 'appearance\\x85': 96738,\n",
       " 'jivetalking': 72700,\n",
       " 'jinks': 26286,\n",
       " 'reuhl': 114741,\n",
       " 'bolstered': 26480,\n",
       " 'graps': 114843,\n",
       " 'straddled': 63216,\n",
       " 'ham': 4550,\n",
       " 'animaniacs': 33403,\n",
       " 'ono': 44200,\n",
       " 'responsible': 1850,\n",
       " 'internships': 106882,\n",
       " 'pyrokineticists': 76596,\n",
       " 'shoulders': 5525,\n",
       " 'donig': 93145,\n",
       " 'aargh': 41561,\n",
       " 'ayre': 84884,\n",
       " 'devastatingly': 23246,\n",
       " 'severed': 6899,\n",
       " 'consoling': 36023,\n",
       " 'meance': 86641,\n",
       " 'bodypress': 90411,\n",
       " 'integral': 10184,\n",
       " 'ismail': 42708,\n",
       " 'drapes': 27510,\n",
       " 'ppppuuuulllleeeeeez': 98261,\n",
       " 'deathrap': 82443,\n",
       " 'buggy': 22562,\n",
       " \"hook's\": 96394,\n",
       " 'justify': 4301,\n",
       " 'sabea': 71551,\n",
       " 'wickham': 38668,\n",
       " 'furnishes': 60704,\n",
       " 'mopsy': 44427,\n",
       " 'unfazed': 40008,\n",
       " \"'cannonball\": 112414,\n",
       " 'absorbs': 24957,\n",
       " 'imotep': 90189,\n",
       " \"tromeo's\": 55517,\n",
       " 'gator': 16299,\n",
       " \"'children's\": 123318,\n",
       " '373': 106880,\n",
       " 'humanizes': 36447,\n",
       " \"miners'\": 80446,\n",
       " 'latin': 5356,\n",
       " 'get': 76,\n",
       " 'gilseg': 114594,\n",
       " \"hardbody's\": 62413,\n",
       " 'druing': 95322,\n",
       " 'hussman': 69049,\n",
       " 'ecstasies': 107090,\n",
       " 'santhanam': 71759,\n",
       " 'damned': 4844,\n",
       " 'chillout': 92405,\n",
       " 'oftentimes': 19003,\n",
       " 'agnew': 53991,\n",
       " 'utilising': 35793,\n",
       " 'verily': 56797,\n",
       " 'tweeness': 96986,\n",
       " \"bane'\": 109753,\n",
       " 'weberian': 78862,\n",
       " 'snuff': 6107,\n",
       " 'nobuo': 69419,\n",
       " 'bnl': 41766,\n",
       " 'loft': 17823,\n",
       " 'ponyos': 106393,\n",
       " 'onus': 53995,\n",
       " 'satori': 108081,\n",
       " 'cool´s': 67363,\n",
       " 'sevizia': 51931,\n",
       " 'togetherness': 26824,\n",
       " 'sacrilegiously': 117372,\n",
       " \"'deathbed\": 115845,\n",
       " 'proofread': 53946,\n",
       " '10star': 83040,\n",
       " 'martix': 92217,\n",
       " \"dorma'\": 91378,\n",
       " 'intensional': 59457,\n",
       " \"wrestler's\": 97745,\n",
       " \"location's\": 102895,\n",
       " 'vaunted': 41931,\n",
       " 'kate': 2397,\n",
       " 'pubic': 24560,\n",
       " 'fecund': 60183,\n",
       " 'copywriter': 92496,\n",
       " 'mormondom': 93938,\n",
       " \"garofalo's\": 60650,\n",
       " '30ish': 64404,\n",
       " 'expends': 69798,\n",
       " 'revoltingly': 52765,\n",
       " 'chandon': 35089,\n",
       " 'kindle': 49724,\n",
       " 'places\\x85you': 83299,\n",
       " 'bichunmoo': 110610,\n",
       " \"wants'\": 123006,\n",
       " \"peluso's\": 103273,\n",
       " 'deserts': 12622,\n",
       " 'caldera': 109033,\n",
       " 'maggi': 50438,\n",
       " \"'musical'\": 65881,\n",
       " 'ufos': 19280,\n",
       " 'judaai': 91838,\n",
       " 'belivably': 84218,\n",
       " 'lakes': 28901,\n",
       " 'carville': 117270,\n",
       " 'metschurat': 40265,\n",
       " 'chefs': 22468,\n",
       " 'hariett': 91407,\n",
       " 'xy': 66135,\n",
       " 'bosarian': 122050,\n",
       " 'tropicana': 47318,\n",
       " 'brit': 9361,\n",
       " \"crow's\": 47850,\n",
       " 'cicra': 109801,\n",
       " 'dankness': 89471,\n",
       " 'pensacolians': 60287,\n",
       " 'dogma95': 99182,\n",
       " 'unsubstantial': 52469,\n",
       " 'longhetti': 98355,\n",
       " 'hoenack': 70219,\n",
       " 'jameson': 14772,\n",
       " 'enduring': 7581,\n",
       " 'drusse': 43593,\n",
       " 'naturist': 83372,\n",
       " 'alliance': 9240,\n",
       " \"'reconstituirea'\": 111515,\n",
       " 'blest': 109349,\n",
       " 'tomreynolds2004': 76017,\n",
       " 'curmudgeon': 26109,\n",
       " 'sonatas': 119639,\n",
       " 'direst': 83181,\n",
       " 'excess': 6786,\n",
       " 'secondtime': 119386,\n",
       " 'lando': 22140,\n",
       " 'automagically': 116241,\n",
       " 'cattle': 6287,\n",
       " 'murderball': 57849,\n",
       " 'yanno': 97171,\n",
       " 'polarisdib': 16407,\n",
       " 'neorealism': 29690,\n",
       " 'lodging': 36035,\n",
       " \"herapheri's\": 89261,\n",
       " 'dukey': 62793,\n",
       " \"cola's\": 98087,\n",
       " \"lieberman's\": 38552,\n",
       " 'olen': 12476,\n",
       " 'thenewamerican': 94538,\n",
       " 'merk': 13696,\n",
       " 'plaiting': 103851,\n",
       " \"lear'\": 68230,\n",
       " 'sachio': 117234,\n",
       " 'improvisation': 13463,\n",
       " 'quaintness': 58259,\n",
       " 'tyte': 67043,\n",
       " \"'christian'\": 56889,\n",
       " 'monologs': 104171,\n",
       " 'camerlingo': 47948,\n",
       " 'starscape': 115105,\n",
       " 'baldness': 53667,\n",
       " \"mimsy's\": 101147,\n",
       " 'freeeeee': 95363,\n",
       " 'remnants': 19006,\n",
       " 'woodgrain': 74353,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the tokenizer to convert all texts in the training-set to lists of these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they\\'ll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it\\'s like to be homeless? That is Goddard Bolt\\'s lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet\\'s on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can\\'t step off the sidewalk. He\\'s given the nickname Pepto by a vagrant after it\\'s written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They\\'re survivors. Bolt isn\\'t. He\\'s not used to reaching mutual agreements like he once did when being rich where it\\'s fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn\\'t necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks\\' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it\\'s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don\\'t know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### here This text corresponds to the following list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  38,   14,  744, 3506,   45,   75,   32, 1771,   15,  153,   18,\n",
       "        110,    3, 1344,    5,  343,  143,   20,    1,  920,   12,   70,\n",
       "        281, 1228,  395,   35,  115,  267,   36,  166,    5,  368,  158,\n",
       "         38, 2058,   15,    1,  504,   88,   83,  101,    4,    1, 4339,\n",
       "         14,   39,    3,  432, 1148,  136, 8697,   42,  177,  138,   14,\n",
       "       2791,    1,  295,   20, 5276,  351,    5, 3029, 2310,    1,   38,\n",
       "       8697,   43, 3611,   26,  365,    5,  127,   53,   20,    1, 2032,\n",
       "          7,    7,   18,   48,   43,   22,   70,  358,    3, 2343,    5,\n",
       "        420,   20,    1, 2032,   15,    3, 3346,  208,    1,   22,  281,\n",
       "         66,   36,    3,  344,    1,  728,  730,    3, 3864, 1320,   20,\n",
       "          1, 1543,    3, 1293,    2,  267,   22,  281, 2734,    5,   63,\n",
       "         48,   44,   37,    5,   26, 4339,   12,    6, 2079,    7,    7,\n",
       "       3425, 2891,   35, 4446,   35,  405,   14,  297,    3,  986,  128,\n",
       "         35,   45,  267,    8,    1,  181,  366, 6951,    5,   94,    3,\n",
       "       2343,   16,    3, 7017, 3090,    5,   63,   43,   28,   67,  420,\n",
       "          8,    1, 2032,   15, 3082,  483,  208,    1,   43, 2802,   28,\n",
       "         67,   77,   48,   28,  487,   16,    3,  731, 1146,    4,  232,\n",
       "         51, 4161,    1,   20,  117,    6, 1334,   20,    1,  920,   16,\n",
       "          3,   20,   24, 4086,    5,   24,  170,  831,  117,   28,  185,\n",
       "       1562,  122,    1, 7951,  237,  358,    1,   31,    3,  100,   44,\n",
       "        407,   20,   24, 9597,  117,  911,   79,  102,  585,    3,  257,\n",
       "         31,    1,  389,    4, 5176, 2137, 4636,   32, 1222, 3303,   35,\n",
       "        189, 4287,  159, 2320,   40,  344,    2,   40, 8527, 6229, 1955,\n",
       "       4910,    2, 7720, 2618,   35,   23,  472,  328,    5,    1, 2032,\n",
       "        501, 4392,  213,  237,   21,  328,    5, 4805, 6768,   37,   28,\n",
       "        281,  115,   50,  109,  986,  117,   44,  557,   38, 2574,  505,\n",
       "         38,   26,  531,    7,    7,  136,    1,  112, 1906,  201, 5176,\n",
       "          2,  292, 1731,    5,  111,   10,  255,  114, 4541,    5,   26,\n",
       "         27,    4, 3425,  104,  117, 2557,    5,  109,    3,  202,    9,\n",
       "        276,    3, 4317,  486, 1107,    5,   24, 2347,  158,  138,   14,\n",
       "       8161,  186, 3889,   38,   15,    1,  504,    5,  119,   48,   44,\n",
       "         37,  263,  137, 4737,  159, 2320,    9,    1,  365,  254,   38,\n",
       "         20,    1,   79,  524,  232,    3,  364, 2343,   37,   29,  986,\n",
       "         83,   77,   50,   33,   89,  118,   48,    5,   77,   16,   65,\n",
       "        290,  273,   33,  142,  197,    9,    5,    1, 4339,  298,    4,\n",
       "        783,    9,   37,  290,    7,    7,   38,  273,   11,   19,   80,\n",
       "       5541,   22,    5,  343,  400])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also need to convert the texts in the test-set to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Padding and Truncating Data\n",
    "\n",
    "we use the length of the longest sequence in the data-set, then we are wasting a lot of memory. This is particularly important for larger data-sets. \n",
    "\n",
    "So in order to make a compromise, we will use a sequence-length that covers most sequences in the data-set, and we will then truncate longer sequences and pad shorter sequences.\n",
    "\n",
    "First we count the number of tokens in all the sequences in the data-set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.27716"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens) #The average number of tokens in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2209"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(num_tokens) #The maximum number of tokens in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The max number of tokens we will allow is set to the average plus 2 standard deviations.\n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9453"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens) #This covers about 95% of the data-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When padding or truncating the sequences that have a different length, we need to determine if we want to do this padding or truncating 'pre' or 'post'. If a sequence is truncated, it means that a part of the sequence is simply thrown away. If a sequence is padded, it means that zeros are added to the sequence.\n",
    "\n",
    "So the choice of 'pre' or 'post' can be important because it determines whether we throw away the first or last part of a sequence when truncating, and it determines whether we add zeros to the beginning or end of the sequence when padding. This may confuse the Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 544)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  38,   14,  744, 3506,   45,   75,   32, 1771,   15,  153,   18,\n",
       "        110,    3, 1344,    5,  343,  143,   20,    1,  920,   12,   70,\n",
       "        281, 1228,  395,   35,  115,  267,   36,  166,    5,  368,  158,\n",
       "         38, 2058,   15,    1,  504,   88,   83,  101,    4,    1, 4339,\n",
       "         14,   39,    3,  432, 1148,  136, 8697,   42,  177,  138,   14,\n",
       "       2791,    1,  295,   20, 5276,  351,    5, 3029, 2310,    1,   38,\n",
       "       8697,   43, 3611,   26,  365,    5,  127,   53,   20,    1, 2032,\n",
       "          7,    7,   18,   48,   43,   22,   70,  358,    3, 2343,    5,\n",
       "        420,   20,    1, 2032,   15,    3, 3346,  208,    1,   22,  281,\n",
       "         66,   36,    3,  344,    1,  728,  730,    3, 3864, 1320,   20,\n",
       "          1, 1543,    3, 1293,    2,  267,   22,  281, 2734,    5,   63,\n",
       "         48,   44,   37,    5,   26, 4339,   12,    6, 2079,    7,    7,\n",
       "       3425, 2891,   35, 4446,   35,  405,   14,  297,    3,  986,  128,\n",
       "         35,   45,  267,    8,    1,  181,  366, 6951,    5,   94,    3,\n",
       "       2343,   16,    3, 7017, 3090,    5,   63,   43,   28,   67,  420,\n",
       "          8,    1, 2032,   15, 3082,  483,  208,    1,   43, 2802,   28,\n",
       "         67,   77,   48,   28,  487,   16,    3,  731, 1146,    4,  232,\n",
       "         51, 4161,    1,   20,  117,    6, 1334,   20,    1,  920,   16,\n",
       "          3,   20,   24, 4086,    5,   24,  170,  831,  117,   28,  185,\n",
       "       1562,  122,    1, 7951,  237,  358,    1,   31,    3,  100,   44,\n",
       "        407,   20,   24, 9597,  117,  911,   79,  102,  585,    3,  257,\n",
       "         31,    1,  389,    4, 5176, 2137, 4636,   32, 1222, 3303,   35,\n",
       "        189, 4287,  159, 2320,   40,  344,    2,   40, 8527, 6229, 1955,\n",
       "       4910,    2, 7720, 2618,   35,   23,  472,  328,    5,    1, 2032,\n",
       "        501, 4392,  213,  237,   21,  328,    5, 4805, 6768,   37,   28,\n",
       "        281,  115,   50,  109,  986,  117,   44,  557,   38, 2574,  505,\n",
       "         38,   26,  531,    7,    7,  136,    1,  112, 1906,  201, 5176,\n",
       "          2,  292, 1731,    5,  111,   10,  255,  114, 4541,    5,   26,\n",
       "         27,    4, 3425,  104,  117, 2557,    5,  109,    3,  202,    9,\n",
       "        276,    3, 4317,  486, 1107,    5,   24, 2347,  158,  138,   14,\n",
       "       8161,  186, 3889,   38,   15,    1,  504,    5,  119,   48,   44,\n",
       "         37,  263,  137, 4737,  159, 2320,    9,    1,  365,  254,   38,\n",
       "         20,    1,   79,  524,  232,    3,  364, 2343,   37,   29,  986,\n",
       "         83,   77,   50,   33,   89,  118,   48,    5,   77,   16,   65,\n",
       "        290,  273,   33,  142,  197,    9,    5,    1, 4339,  298,    4,\n",
       "        783,    9,   37,  290,    7,    7,   38,  273,   11,   19,   80,\n",
       "       5541,   22,    5,  343,  400])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXAMPLE\n",
    "\n",
    "np.array(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         38,   14,  744, 3506,   45,   75,   32, 1771,   15,  153,   18,\n",
       "        110,    3, 1344,    5,  343,  143,   20,    1,  920,   12,   70,\n",
       "        281, 1228,  395,   35,  115,  267,   36,  166,    5,  368,  158,\n",
       "         38, 2058,   15,    1,  504,   88,   83,  101,    4,    1, 4339,\n",
       "         14,   39,    3,  432, 1148,  136, 8697,   42,  177,  138,   14,\n",
       "       2791,    1,  295,   20, 5276,  351,    5, 3029, 2310,    1,   38,\n",
       "       8697,   43, 3611,   26,  365,    5,  127,   53,   20,    1, 2032,\n",
       "          7,    7,   18,   48,   43,   22,   70,  358,    3, 2343,    5,\n",
       "        420,   20,    1, 2032,   15,    3, 3346,  208,    1,   22,  281,\n",
       "         66,   36,    3,  344,    1,  728,  730,    3, 3864, 1320,   20,\n",
       "          1, 1543,    3, 1293,    2,  267,   22,  281, 2734,    5,   63,\n",
       "         48,   44,   37,    5,   26, 4339,   12,    6, 2079,    7,    7,\n",
       "       3425, 2891,   35, 4446,   35,  405,   14,  297,    3,  986,  128,\n",
       "         35,   45,  267,    8,    1,  181,  366, 6951,    5,   94,    3,\n",
       "       2343,   16,    3, 7017, 3090,    5,   63,   43,   28,   67,  420,\n",
       "          8,    1, 2032,   15, 3082,  483,  208,    1,   43, 2802,   28,\n",
       "         67,   77,   48,   28,  487,   16,    3,  731, 1146,    4,  232,\n",
       "         51, 4161,    1,   20,  117,    6, 1334,   20,    1,  920,   16,\n",
       "          3,   20,   24, 4086,    5,   24,  170,  831,  117,   28,  185,\n",
       "       1562,  122,    1, 7951,  237,  358,    1,   31,    3,  100,   44,\n",
       "        407,   20,   24, 9597,  117,  911,   79,  102,  585,    3,  257,\n",
       "         31,    1,  389,    4, 5176, 2137, 4636,   32, 1222, 3303,   35,\n",
       "        189, 4287,  159, 2320,   40,  344,    2,   40, 8527, 6229, 1955,\n",
       "       4910,    2, 7720, 2618,   35,   23,  472,  328,    5,    1, 2032,\n",
       "        501, 4392,  213,  237,   21,  328,    5, 4805, 6768,   37,   28,\n",
       "        281,  115,   50,  109,  986,  117,   44,  557,   38, 2574,  505,\n",
       "         38,   26,  531,    7,    7,  136,    1,  112, 1906,  201, 5176,\n",
       "          2,  292, 1731,    5,  111,   10,  255,  114, 4541,    5,   26,\n",
       "         27,    4, 3425,  104,  117, 2557,    5,  109,    3,  202,    9,\n",
       "        276,    3, 4317,  486, 1107,    5,   24, 2347,  158,  138,   14,\n",
       "       8161,  186, 3889,   38,   15,    1,  504,    5,  119,   48,   44,\n",
       "         37,  263,  137, 4737,  159, 2320,    9,    1,  365,  254,   38,\n",
       "         20,    1,   79,  524,  232,    3,  364, 2343,   37,   29,  986,\n",
       "         83,   77,   50,   33,   89,  118,   48,    5,   77,   16,   65,\n",
       "        290,  273,   33,  142,  197,    9,    5,    1, 4339,  298,    4,\n",
       "        783,    9,   37,  290,    7,    7,   38,  273,   11,   19,   80,\n",
       "       5541,   22,    5,  343,  400])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after padding\n",
    "\n",
    "x_train_pad[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Inverse Map\n",
    "\n",
    "For some strange reason, the Keras implementation of a tokenizer does not seem to have the inverse mapping from integer-tokens back to words, which is needed to reconstruct text-strings from lists of tokens. So we make that mapping here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they\\'ll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it\\'s like to be homeless? That is Goddard Bolt\\'s lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet\\'s on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can\\'t step off the sidewalk. He\\'s given the nickname Pepto by a vagrant after it\\'s written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They\\'re survivors. Bolt isn\\'t. He\\'s not used to reaching mutual agreements like he once did when being rich where it\\'s fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn\\'t necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks\\' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it\\'s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don\\'t know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, this is the original text from the data-set:\n",
    "\n",
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"or as george stated has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school work or vote for the matter most people think of the homeless as just a lost cause while worrying about things such as racism the war on iraq kids to succeed technology the or worrying if they'll be next to end up on the streets br br but what if you were given a bet to live on the streets for a month without the you once had from a home the entertainment sets a bathroom pictures on the wall a computer and everything you once treasure to see what it's like to be homeless that is lesson br br mel brooks who directs who stars as plays a rich man who has everything in the world until deciding to make a bet with a sissy rival to see if he can live in the streets for thirty days without the if succeeds he can do what he wants with a future project of making more buildings the on where is thrown on the street with a on his leg to his every move where he can't step off the sidewalk he's given the by a after it's written on his forehead where meets other characters including a woman by the name of molly ann warren an ex dancer who got divorce before losing her home and her pals sailor howard morris and teddy wilson who are already used to the streets they're survivors isn't he's not used to reaching mutual like he once did when being rich where it's fight or flight kill or be killed br br while the love connection between molly and wasn't necessary to plot i found life stinks to be one of mel films where prior to being a comedy it shows a tender side compared to his slapstick work such as blazing young frankenstein or for the matter to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money maybe they should give it to the homeless instead of using it like money br br or maybe this film will inspire you to help others\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now add the first Gated Recurrent Unit (GRU) to the network. \n",
    "#This will have 16 outputs. Because we will add a second GRU after this one, \n",
    "#we need to return sequences of data because the next GRU expects sequences as its input.\n",
    "\n",
    "model.add(GRU(units=16, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adds the second GRU with 8 output units. \n",
    "#This will be followed by another GRU so it must also return sequences.\n",
    "\n",
    "\n",
    "model.add(GRU(units=8, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a fully-connected / dense layer which computes a value between 0.0 and 1.0 that will be used as the classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sigmoid activation ()\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer with the given learning-rate.\n",
    "\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the Keras model so it is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 544, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 544, 16)           1200      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 544, 8)            600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 81,963\n",
      "Trainable params: 81,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Training the Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 466s 20ms/step - loss: 0.6954 - acc: 0.4997 - val_loss: 0.7274 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 496s 21ms/step - loss: 0.6918 - acc: 0.5263 - val_loss: 0.7397 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 533s 22ms/step - loss: 0.6723 - acc: 0.6041 - val_loss: 0.7253 - val_acc: 0.2640\n",
      "Wall time: 25min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x138881fcba8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train_pad, y_train,\n",
    "          validation_split=0.05, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Test-Set\n",
    "Now that the model has been trained we can calculate its classification accuracy on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 170s 7ms/step\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = model.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.25%\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Mis-Classified Text\n",
    "\n",
    "In order to show an example of mis-classified text, we first calculate the predicted sentiment for the first 1000 texts in the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = model.predict(x=x_test_pad[0:1000])\n",
    "y_pred = y_pred.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predicted numbers fall between 0.0 and 1.0. We use a cutoff / threshold and say that all values above 0.5 are taken to be 1.0 and all values below 0.5 are taken to be 0.0. This gives us a predicted \"class\" of either 0.0 or 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true \"class\" for the first 1000 texts in the test-set are needed for comparison.\n",
    "\n",
    "cls_true = np.array(y_test[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then get indices for all the texts that were incorrectly classified \n",
    "#by comparing all the \"classes\" of these two arrays.\n",
    "\n",
    "incorrect = np.where(cls_pred != cls_true)\n",
    "incorrect = incorrect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the first mis-classified text. We will use its index several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = incorrect[0]\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mis-classified text is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'George Armstrong Custer is known through history as an inept General who led his rgiment to their death at the battle of Little Big Horn. \"They Died with their boots on,\" paints a different picture of General Custer. In this movie he is portrayed as a Flamboyant soldier whose mistakes, and misdeeds are mostly ue to his love for adventure.<br /><br />Errol Flynn plays George Armstrong Custer who we first meet as an over confident recruit at West Point. Custer quickily distinguishes himself from other cadets as beeing a poor student who always seems to be in trouble. Somehow this never appears to bother Custer and only seems to confuse him as he genuinely does not know how he gets into such predicaments. In spite of his poor standing, he eventualy graduates and becomes an officer in the United States Army. Through an error, Custer receives a promotion in rank. Before this can be corrected, he leads a Union regiment into battle against the Confederates. His campaign is successful and Custer becomes an unlikely national hero. Custer returns to his hometown, marries his sweetheart, Libby who is played by Olivia De Havilland. Libby is a very supportive understanding wife who steadfastly stays by his side and follows him into the frontier as he assumes leadership of the Seventh Regiment of the Cavalry. Custer becomes a man of honor who strives to keep peace with the Native Americans. To prove his intentions, he enters into a treaty with Crazy Horse, the leader of the Sioux . When that treaty is jeopardized by a conspiracy to spread a false rumor of gold being found in the Black Hills, Custer sacrifices his own life as well as the lives of the men under his command to prevent the slaughter of thousands of innocent settlers.<br /><br />Errol Flynn dominates each scene in which he appears. He successfully portrays Custer as being flamboyant, arrogant, romantic and funny depending on the mood of the scene. Olivia De Havilland\\'s depiction of Libby Bacon Custer as the love of his life lets us see his tender, more gentle side. The Chemistry between DeHavilland and Flynn, who had acted together in several other movies, is so smooth and it almost makes the viewer feel like they are playing themselves and not the parts of Custer and his wife. The other actors portrayals of their characters truly enhance the performances of Flynn and De Havilland. Anthony Quinn as Crazy Horse, Sidney Greenstreet as General Winfield Scott , Arthur Kennedy as Edward Sharp are among the other actors whose roles have made this movie entertaining.<br /><br />The reviewer would rate this a 4 star movie. While it is not historically accurate, it is very entertaining. The movie has a little bit of everything. It has adventure, comedy and romance, so it appeals to a large variety of audiences. The casting of the characters is excellent and the actors give believable performances which makes you forget it is largely based on fiction instead of fact. The reviewer especially likes that the Native Americans were not shown to be the bad guys but just showed them as wanting to protect their sacred land.<br /><br />'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = x_test_text[idx]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44949216"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the predicted and true classes for the text:\n",
    "\n",
    "y_pred[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_true[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data\n",
    "\n",
    "Let us try and classify new texts that we make up. Some of these are obvious, while others use negation and sarcasm to try and confuse the model into mis-classifying the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "text2 = \"Good movie!\"\n",
    "text3 = \"Maybe I like this movie.\"\n",
    "text4 = \"Meh ...\"\n",
    "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
    "text6 = \"Bad movie!\"\n",
    "text7 = \"Not a good movie!\"\n",
    "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "texts = [text1, text2, text3, text4, text5, text6, text7, text8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now We first convert these texts to arrays of integer-tokens because that is needed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 544)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To input texts with different lengths into the model, we also need to pad and truncate them.\n",
    "\n",
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "tokens_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55837977],\n",
       "       [0.55845195],\n",
       "       [0.5578019 ],\n",
       "       [0.5584939 ],\n",
       "       [0.5581721 ],\n",
       "       [0.5568378 ],\n",
       "       [0.55841994],\n",
       "       [0.5292616 ]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokens_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value close to 0.0 means a negative sentiment and a value close to 1.0 means a positive sentiment. These numbers will vary every time you train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "The model cannot work on integer-tokens directly, because they are integer values that may range between 0 and the number of words in our vocabulary, e.g. 10000. So we need to convert the integer-tokens into vectors of values that are roughly between -1.0 and 1.0 which can be used as input to a neural network.\n",
    "\n",
    "This mapping from integer-tokens to real-valued vectors is also called an \"embedding\". It is essentially just a matrix where each row contains the vector-mapping of a single token. This means we can quickly lookup the mapping of each integer-token by simply using the token as an index into the matrix. The embeddings are learned along with the rest of the model during training.\n",
    "\n",
    "Ideally the embedding would learn a mapping where words that are similar in meaning also have similar embedding-values. Let us investigate if that has happened here.\n",
    "\n",
    "First we need to get the embedding-layer from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get the embedding-layer from the model:\n",
    "\n",
    "layer_embedding = model.get_layer('layer_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then get the weights used for the mapping done by the embedding-layer.\n",
    "\n",
    "weights_embedding = layer_embedding.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the weights are actually just a matrix with the number of words in the \n",
    "#vocabulary times the vector length for each embedding.\n",
    "\n",
    "weights_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us get the integer-token for the word 'good', which is just an index into the vocabulary.\n",
    "\n",
    "token_good = tokenizer.word_index['good']\n",
    "token_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us also get the integer-token for the word 'great'.\n",
    "\n",
    "token_great = tokenizer.word_index['great']\n",
    "token_great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These integertokens may be far apart and will depend on the frequency of those words in the data-set.\n",
    "\n",
    "Now let us compare the vector-embeddings for the words 'good' and 'great'. Several of these values are similar, although some values are quite different. Note that these values will change every time you train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.978171  , 0.5937643 , 0.43536103, 0.5319829 , 0.08964466,\n",
       "       0.6342454 , 0.5165227 , 0.6295241 ], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05571837,  0.65573746, -0.14962612,  0.7024609 , -0.17788634,\n",
       "        0.39176622, -0.01695074,  0.9315679 ], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_great]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Similarly, we can compare the embeddings for the words 'bad' and 'horrible'.\n",
    "\n",
    "\n",
    "token_bad = tokenizer.word_index['bad']\n",
    "token_horrible = tokenizer.word_index['horrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.86898375, -0.57986397,  0.45750946,  0.11470479,  0.9568155 ,\n",
       "        0.24495511,  0.15580253, -0.3214397 ], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7288629 ,  0.48718736,  0.5936784 , -0.35243008,  0.77482307,\n",
       "        0.35306722,  0.3102466 ,  0.39246282], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_horrible]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Sorted Words\n",
    "\n",
    "We can also sort all the words in the vocabulary according to their \"similarity\" in the embedding-space. We want to see if words that have similar embedding-vectors also have similar meanings.\n",
    "\n",
    "Similarity of embedding-vectors can be measured by different metrics, e.g. Euclidean distance or cosine distance.\n",
    "\n",
    "We have a helper-function for calculating these distances and printing the words in sorted order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sorted_words(word, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Print the words in the vocabulary sorted according to their\n",
    "    embedding-distance to the given word.\n",
    "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the token (i.e. integer ID) for the given word.\n",
    "    token = tokenizer.word_index[word]\n",
    "\n",
    "    # Get the embedding for the given word. Note that the\n",
    "    # embedding-weight-matrix is indexed by the word-tokens\n",
    "    # which are integer IDs.\n",
    "    embedding = weights_embedding[token]\n",
    "\n",
    "    # Calculate the distance between the embeddings for\n",
    "    # this word and all other words in the vocabulary.\n",
    "    distances = cdist(weights_embedding, [embedding],\n",
    "                      metric=metric).T[0]\n",
    "    \n",
    "    # Get an index sorted according to the embedding-distances.\n",
    "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
    "    sorted_index = np.argsort(distances)\n",
    "    \n",
    "    # Sort the embedding-distances.\n",
    "    sorted_distances = distances[sorted_index]\n",
    "    \n",
    "    # Sort all the words in the vocabulary according to their\n",
    "    # embedding-distance. This is a bit excessive because we\n",
    "    # will only print the top and bottom words.\n",
    "    sorted_words = [inverse_map[token] for token in sorted_index\n",
    "                    if token != 0]\n",
    "\n",
    "    # Helper-function for printing words and embedding-distances.\n",
    "    def _print_words(words, distances):\n",
    "        for word, distance in zip(words, distances):\n",
    "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
    "\n",
    "    # Number of words to print from the top and bottom of the list.\n",
    "    k = 10\n",
    "\n",
    "    print(\"Distance from '{0}':\".format(word))\n",
    "\n",
    "    # Print the words with smallest embedding-distance.\n",
    "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
    "\n",
    "    print(\"...\")\n",
    "\n",
    "    # Print the words with highest embedding-distance.\n",
    "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then print the words that are near and far from the word 'great' in terms of their vector-embeddings. Note that these may change each time you train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'great':\n",
      "0.000 - great\n",
      "0.045 - definitely\n",
      "0.067 - excellent\n",
      "0.081 - oops\n",
      "0.086 - beautiful\n",
      "0.086 - adventure\n",
      "0.091 - esteem\n",
      "0.095 - slipped\n",
      "0.095 - months\n",
      "0.097 - masterpiece\n",
      "...\n",
      "1.152 - turd\n",
      "1.157 - stinker\n",
      "1.176 - avoid\n",
      "1.235 - skip\n",
      "1.245 - money\n",
      "1.246 - d\n",
      "1.328 - waste\n",
      "1.338 - awful\n",
      "1.338 - 1\n",
      "1.360 - bad\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('great', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'worst':\n",
      "0.000 - worst\n",
      "0.086 - offensive\n",
      "0.092 - mildly\n",
      "0.096 - happens\n",
      "0.103 - idiotic\n",
      "0.110 - teenagers\n",
      "0.112 - evan\n",
      "0.116 - amusing\n",
      "0.119 - failing\n",
      "0.120 - misses\n",
      "...\n",
      "1.082 - times\n",
      "1.092 - casts\n",
      "1.108 - finding\n",
      "1.108 - cedric\n",
      "1.116 - developing\n",
      "1.118 - since\n",
      "1.124 - realization\n",
      "1.127 - captured\n",
      "1.160 - yesterday\n",
      "1.216 - relentless\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('worst', metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basic methods for doing Natural Language Processing (NLP) using a Recurrent Neural Network with integer-tokens and an embedding layer. This was used to do sentiment analysis of movie reviews from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
